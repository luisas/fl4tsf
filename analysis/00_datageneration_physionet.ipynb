{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f2639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.generate_timeseries import Periodic_1d\n",
    "import torch\n",
    "from collections import Counter\n",
    "from matplotlib import cm\n",
    "from torch.distributions import uniform\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "os.chdir(\"/Users/luisa/Desktop/nygc/cluster/projects/fl4tsf/bin\")\n",
    "sys.path.append(os.getcwd()) \n",
    "sns.set(style='whitegrid')\n",
    "from lib.dataset_utils import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from plot_utils import read_loss_file\n",
    "from lib.physionet import PhysioNet, variable_time_collate_fn, get_data_min_max\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import DataLoader\n",
    "import lib.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = \"/Users/luisa/Desktop/nygc/cluster/projects/fl4tsf/data\"\n",
    "prefix = \"physionet\"\n",
    "batch_size = 64\n",
    "classif = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71ddcd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../data/physionet/PhysioNet/raw/Outcomes-a.txt\n",
      "Using downloaded and verified file: ../data/physionet/PhysioNet/raw/set-a.tar.gz?download\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;66;03m# number of samples to use from the dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_dataset_obj \u001b[38;5;241m=\u001b[39m PhysioNet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/physionet\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      9\u001b[0m                                 quantization \u001b[38;5;241m=\u001b[39m quantization,\n\u001b[1;32m     10\u001b[0m                                 download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m10000\u001b[39m, n), \n\u001b[1;32m     11\u001b[0m                                 device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Use custom collate_fn to combine samples with arbitrary time observations.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Returns the dataset along with mask and time steps\u001b[39;00m\n\u001b[1;32m     14\u001b[0m test_dataset_obj \u001b[38;5;241m=\u001b[39m PhysioNet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/physionet\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     15\u001b[0m                                 quantization \u001b[38;5;241m=\u001b[39m quantization,\n\u001b[1;32m     16\u001b[0m                                 download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m10000\u001b[39m,n), \n\u001b[1;32m     17\u001b[0m                                 device \u001b[38;5;241m=\u001b[39m device)\n",
      "File \u001b[0;32m~/Desktop/nygc/cluster/projects/fl4tsf/bin/lib/physionet.py:89\u001b[0m, in \u001b[0;36mPhysioNet.__init__\u001b[0;34m(self, root, train, download, quantization, n_samples, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization \u001b[38;5;241m=\u001b[39m quantization\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 89\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m     92\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/nygc/cluster/projects/fl4tsf/bin/lib/physionet.py:143\u001b[0m, in \u001b[0;36mPhysioNet.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m download_url(url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_folder, filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m tar \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_folder, filename), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr:gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m tar\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_folder)\n\u001b[1;32m    144\u001b[0m tar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(filename))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/tarfile.py:2302\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner, filter)\u001b[0m\n\u001b[1;32m   2297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m   2298\u001b[0m         \u001b[38;5;66;03m# For directories, delay setting attributes until later,\u001b[39;00m\n\u001b[1;32m   2299\u001b[0m         \u001b[38;5;66;03m# since permissions can interfere with extraction and\u001b[39;00m\n\u001b[1;32m   2300\u001b[0m         \u001b[38;5;66;03m# extracting contents can reset mtime.\u001b[39;00m\n\u001b[1;32m   2301\u001b[0m         directories\u001b[38;5;241m.\u001b[39mappend(tarinfo)\n\u001b[0;32m-> 2302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_one(tarinfo, path, set_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir(),\n\u001b[1;32m   2303\u001b[0m                       numeric_owner\u001b[38;5;241m=\u001b[39mnumeric_owner)\n\u001b[1;32m   2305\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2306\u001b[0m directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mname, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/tarfile.py:2365\u001b[0m, in \u001b[0;36mTarFile._extract_one\u001b[0;34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2364\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_member(tarinfo, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, tarinfo\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   2366\u001b[0m                          set_attrs\u001b[38;5;241m=\u001b[39mset_attrs,\n\u001b[1;32m   2367\u001b[0m                          numeric_owner\u001b[38;5;241m=\u001b[39mnumeric_owner)\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_fatal_error(e)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/tarfile.py:2448\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, tarinfo\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n\u001b[0;32m-> 2448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakefile(tarinfo, targetpath)\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m   2450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/tarfile.py:2502\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2500\u001b[0m     target\u001b[38;5;241m.\u001b[39mtruncate()\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2502\u001b[0m     copyfileobj(source, target, tarinfo\u001b[38;5;241m.\u001b[39msize, ReadError, bufsize)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/tarfile.py:257\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    254\u001b[0m     dst\u001b[38;5;241m.\u001b[39mwrite(buf)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remainder \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 257\u001b[0m     buf \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread(remainder)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) \u001b[38;5;241m<\u001b[39m remainder:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/gzip.py:324\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread(size)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/gzip.py:547\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 547\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompressed file ended before the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    548\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend-of-stream marker was reached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc \u001b[38;5;241m=\u001b[39m zlib\u001b[38;5;241m.\u001b[39mcrc32(uncompress, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(uncompress)\n",
      "\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Physionet Dataset\n",
    "#################################################################\n",
    "quantization = 0.1 # quantization of time stamps in 0.1 hours\n",
    "n = 10000 # number of samples to use from the dataset\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset_obj = PhysioNet('../data/physionet', train=True, \n",
    "                                quantization = quantization,\n",
    "                                download=True, n_samples = min(10000, n), \n",
    "                                device = device)\n",
    "# Use custom collate_fn to combine samples with arbitrary time observations.\n",
    "# Returns the dataset along with mask and time steps\n",
    "test_dataset_obj = PhysioNet('../data/physionet', train=False, \n",
    "                                quantization = quantization,\n",
    "                                download=True, n_samples = min(10000,n), \n",
    "                                device = device)\n",
    "\n",
    "# Combine and shuffle samples from physionet Train and physionet Test\n",
    "total_dataset = train_dataset_obj[:len(train_dataset_obj)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29766e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not classif:\n",
    "    # Concatenate samples from original Train and Test sets\n",
    "    # Only 'training' physionet samples are have labels. Therefore, if we do classifiction task, we don't need physionet 'test' samples.\n",
    "    total_dataset = total_dataset + test_dataset_obj[:len(test_dataset_obj)]\n",
    "\n",
    "# Shuffle and split\n",
    "train_data, test_data = model_selection.train_test_split(total_dataset, train_size= 0.8, \n",
    "    random_state = 42, shuffle = True)\n",
    "\n",
    "record_id, tt, vals, mask, labels = train_data[0]\n",
    "\n",
    "n_samples = len(total_dataset)\n",
    "input_dim = vals.size(-1)\n",
    "\n",
    "batch_size = min(min(len(train_dataset_obj),batch_size), n)\n",
    "data_min, data_max = get_data_min_max(total_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size= batch_size, shuffle=False, \n",
    "    collate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
    "        data_min = data_min, data_max = data_max))\n",
    "test_dataloader = DataLoader(test_data, batch_size = n_samples, shuffle=False, \n",
    "    collate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"test\",\n",
    "        data_min = data_min, data_max = data_max))\n",
    "\n",
    "attr_names = train_dataset_obj.params\n",
    "data_objects = {\"dataset_obj\": train_dataset_obj, \n",
    "            \"train_dataloader\": utils.inf_generator(train_dataloader), \n",
    "            \"test_dataloader\": utils.inf_generator(test_dataloader),\n",
    "            \"input_dim\": input_dim,\n",
    "            \"n_train_batches\": len(train_dataloader),\n",
    "            \"n_test_batches\": len(test_dataloader),\n",
    "            \"attr\": attr_names, #optional\n",
    "            \"classif_per_tp\": False, #optional\n",
    "            \"n_labels\": 1} #optional"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
