{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5459e0e5",
   "metadata": {},
   "source": [
    "## Centralized training\n",
    "\n",
    "- Explore hyperparameters \n",
    "- Check that the model can learn in a centralized setting (setup sanity check)\n",
    "- Get approximately the global centralized loss value it can reach. To be compared with federated setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f55c6292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 11:55:46,055\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/luisa/Desktop/nygc/cluster/projects/fl4tsf\")\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "from plot_utils import plot_n_outputs\n",
    "from flower.task import Net\n",
    "\n",
    "\n",
    "centralized_training_loss = glob.glob(f\"results_centralized_hyperparam_new/**/centralized_training/**/loss_per_epoch.csv\", recursive=True)\n",
    "print(f\"Found {len(centralized_training_loss)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "957b1f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique learning rates: [0.1   0.001 0.01 ]\n",
      "Unique clipping values: [False]\n",
      "Unique batch sizes: [ 64  50  16 100  32]\n"
     ]
    }
   ],
   "source": [
    "# For each file in the list, read the meta.csv file and extract lr  and read in the loss_per_epoch.csv file\n",
    "def read_loss_file(file):\n",
    "    # Read the meta.csv file\n",
    "    meta_file = file.replace(\"loss_per_epoch.csv\", \"meta.csv\")\n",
    "    meta_data = pd.read_csv(meta_file)\n",
    "    lr = meta_data['lr'].item()\n",
    "    clipping = meta_data['gradientclipping'].item()\n",
    "    bs = meta_data['batch_size'].item()\n",
    "    lrdecay = meta_data['lrdecay'].item()\n",
    "    \n",
    "    # Read the loss_per_epoch.csv file\n",
    "    loss_df = pd.read_csv(file)\n",
    "    \n",
    "    # Add the learning rate to the DataFrame\n",
    "    loss_df['lr'] = lr\n",
    "    loss_df['clipping'] = clipping\n",
    "    loss_df['batch_size'] = bs\n",
    "    loss_df['lrdecay'] = lrdecay\n",
    "    \n",
    "    return loss_df\n",
    "# Read all the loss files and concatenate them into a single DataFrame\n",
    "loss_dfs = []\n",
    "for file in centralized_training_loss:\n",
    "    loss_df = read_loss_file(file)\n",
    "    loss_dfs.append(loss_df)\n",
    "loss_df = pd.concat(loss_dfs, ignore_index=True)\n",
    "# Convert the epoch column to a numeric type\n",
    "loss_df[\"epoch\"] = pd.to_numeric(loss_df[\"epoch\"], errors='coerce')\n",
    "# add 1\n",
    "loss_df[\"epoch\"] = loss_df[\"epoch\"] + 1\n",
    "df_centralized_training_loss = loss_df\n",
    "\n",
    "# combine all but loss into hyperparam column\n",
    "def combine_hyperparams(row):\n",
    "    return f\"lr: {row['lr']}, clipping: {row['clipping']}, batch_size: {row['batch_size']}, lrdecay: {row['lrdecay']}\"\n",
    "# apply the function to each row\n",
    "df_centralized_training_loss['hyperparams'] = df_centralized_training_loss.apply(combine_hyperparams, axis=1)\n",
    "\n",
    "# plot uniqure values of lr and clipping\n",
    "unique_lrs = df_centralized_training_loss['lr'].unique()\n",
    "unique_clippings = df_centralized_training_loss['clipping'].unique()\n",
    "unique_batch_sizes = df_centralized_training_loss['batch_size'].unique()\n",
    "print(\"Unique learning rates:\", unique_lrs)\n",
    "print(\"Unique clipping values:\", unique_clippings)\n",
    "print(\"Unique batch sizes:\", unique_batch_sizes)\n",
    "\n",
    "\n",
    "# prepare the data for plotting\n",
    "df_loss_long = pd.concat([\n",
    "    df_centralized_training_loss[['epoch', 'lr', 'batch_size','lrdecay', 'hyperparams', 'train_loss']].rename(columns={'train_loss': 'loss'}).assign(type='train'),\n",
    "    df_centralized_training_loss[['epoch', 'lr', 'batch_size','lrdecay', 'hyperparams', 'val_loss']].rename(columns={'val_loss': 'loss'}).assign(type='val')\n",
    "], ignore_index=True)\n",
    "\n",
    "\n",
    "training = df_loss_long[df_loss_long['type'] == 'train']\n",
    "validation = df_loss_long[df_loss_long['type'] == 'val']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f454ce4",
   "metadata": {},
   "source": [
    "### Check which batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "407115c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/470261691.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = sns.FacetGrid(df_loss_long, col=\"type\", height=4, aspect=1.5, sharey=True)\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"epoch\",\n",
    "    y=\"loss\",\n",
    "    hue=\"batch_size\",\n",
    "    palette=\"Set1\"\n",
    ")\n",
    "\n",
    "g.set_axis_labels(\"Epoch\", \"Loss\")\n",
    "g.set_titles(col_template=\"{col_name} loss\")\n",
    "g.add_legend(title=\"batch size\")\n",
    "g._legend.set_bbox_to_anchor((1.05, 0.5))\n",
    "g.fig.suptitle(\"training vs validation loss by batch size \\n\\n the smallest the batchsize, the better \\n\\n\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "# log scale y axis\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577b88d",
   "metadata": {},
   "source": [
    "### Check which learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "324f71a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/2251822775.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "g = sns.FacetGrid(df_loss_long, col=\"type\", height=4, aspect=1.5, sharey=True)\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"epoch\",\n",
    "    y=\"loss\",\n",
    "    hue=\"lr\",\n",
    "    palette=\"Set1\"\n",
    ")\n",
    "\n",
    "g.set_axis_labels(\"Epoch\", \"Loss\")\n",
    "g.set_titles(col_template=\"{col_name} loss\")\n",
    "g.add_legend(title=\"lr\")\n",
    "g._legend.set_bbox_to_anchor((1.05, 0.5))\n",
    "g.fig.suptitle(\"training vs validation loss by learning rate \\n\\n TODO \\n\\n\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "# plot log scale y axis\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff99fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c9d9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/2912075798.py:19: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "g = sns.FacetGrid(df_loss_long, col=\"type\", height=4, aspect=1.5, sharey=True)\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=\"epoch\",\n",
    "    y=\"loss\",\n",
    "    hue=\"lrdecay\",\n",
    "    palette=\"Set1\"\n",
    ")\n",
    "\n",
    "g.set_axis_labels(\"Epoch\", \"Loss\")\n",
    "g.set_titles(col_template=\"{col_name} loss\")\n",
    "g.add_legend(title=\"lrdecay\")\n",
    "g._legend.set_bbox_to_anchor((1.02, 0.5))\n",
    "g.fig.suptitle(\"training vs validation loss by learning rate \\n\\n TODO \\n\\n\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "# plot log scale y axis\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22848386",
   "metadata": {},
   "source": [
    "### Go more in depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "696ee555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/3108919904.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/3108919904.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/3108919904.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/3108919904.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/3108919904.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loop over each unique batch size\n",
    "for batch_size in sorted(training['batch_size'].unique()):\n",
    "    tr_batch = training[training['batch_size'] == batch_size]\n",
    "\n",
    "    g = sns.FacetGrid(tr_batch, col='lr', col_wrap=3, height=3.5, sharey=True)\n",
    "\n",
    "    g.map_dataframe(\n",
    "        sns.lineplot,\n",
    "        x='epoch',\n",
    "        y='loss',\n",
    "        hue='lrdecay',\n",
    "        palette='Set1',\n",
    "        legend='brief'\n",
    "    )\n",
    "\n",
    "    g.set_axis_labels('Epoch', 'Loss')\n",
    "    g.set_titles(col_template='lr = {col_name}')\n",
    "    g.fig.subplots_adjust(top=0.85)  # leave space for main title\n",
    "    g.fig.suptitle(f\"Batch Size = {batch_size}\", fontsize=12)\n",
    "\n",
    "    # move legend outside\n",
    "    g.add_legend()\n",
    "    g._legend.set_bbox_to_anchor((1.15, 0.6))\n",
    "    # y log\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e18ab6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/4086448026.py:26: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Add title of type \n",
    "\n",
    "# add main title \n",
    "tr_16 = validation[validation['batch_size'] == 16]\n",
    "\n",
    "g = sns.FacetGrid(tr_16, col='lr', col_wrap=3, height=3.5, sharey=True)\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x='epoch',\n",
    "    y='loss',\n",
    "    hue='lrdecay',\n",
    "    palette='Set1',\n",
    "    legend='brief'\n",
    ")\n",
    "\n",
    "g.set_axis_labels('Epoch', 'Loss')\n",
    "g.add_legend()\n",
    "# add legend outside\n",
    "g._legend.set_bbox_to_anchor((1.15, 0.6))\n",
    "g.set_titles(col_template='lr = {col_name}')\n",
    "# log scale y axis\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beda702",
   "metadata": {},
   "source": [
    "# Plot best combination of hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb64496a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lrdecay</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>loss</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 64, lrde...</td>\n",
       "      <td>5239.114258</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 64, lrde...</td>\n",
       "      <td>17432.500000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 64, lrde...</td>\n",
       "      <td>6616.784668</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 64, lrde...</td>\n",
       "      <td>3882.172363</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 64, lrde...</td>\n",
       "      <td>3273.804688</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>296</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 50, lrde...</td>\n",
       "      <td>2631.057373</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>297</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 50, lrde...</td>\n",
       "      <td>2647.614502</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>298</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 50, lrde...</td>\n",
       "      <td>2614.049316</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>299</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 50, lrde...</td>\n",
       "      <td>2644.792725</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999</th>\n",
       "      <td>300</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>lr: 0.1, clipping: False, batch_size: 50, lrde...</td>\n",
       "      <td>2649.737305</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       epoch   lr  batch_size  lrdecay  \\\n",
       "0          1  0.1          64     1.00   \n",
       "1          2  0.1          64     1.00   \n",
       "2          3  0.1          64     1.00   \n",
       "3          4  0.1          64     1.00   \n",
       "4          5  0.1          64     1.00   \n",
       "...      ...  ...         ...      ...   \n",
       "17995    296  0.1          50     0.01   \n",
       "17996    297  0.1          50     0.01   \n",
       "17997    298  0.1          50     0.01   \n",
       "17998    299  0.1          50     0.01   \n",
       "17999    300  0.1          50     0.01   \n",
       "\n",
       "                                             hyperparams          loss   type  \n",
       "0      lr: 0.1, clipping: False, batch_size: 64, lrde...   5239.114258  train  \n",
       "1      lr: 0.1, clipping: False, batch_size: 64, lrde...  17432.500000  train  \n",
       "2      lr: 0.1, clipping: False, batch_size: 64, lrde...   6616.784668  train  \n",
       "3      lr: 0.1, clipping: False, batch_size: 64, lrde...   3882.172363  train  \n",
       "4      lr: 0.1, clipping: False, batch_size: 64, lrde...   3273.804688  train  \n",
       "...                                                  ...           ...    ...  \n",
       "17995  lr: 0.1, clipping: False, batch_size: 50, lrde...   2631.057373  train  \n",
       "17996  lr: 0.1, clipping: False, batch_size: 50, lrde...   2647.614502  train  \n",
       "17997  lr: 0.1, clipping: False, batch_size: 50, lrde...   2614.049316  train  \n",
       "17998  lr: 0.1, clipping: False, batch_size: 50, lrde...   2644.792725  train  \n",
       "17999  lr: 0.1, clipping: False, batch_size: 50, lrde...   2649.737305  train  \n",
       "\n",
       "[18000 rows x 7 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9ca8b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/250150398.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_plot['lr_decay_bs'] = df_plot['lr'].astype(str) + \"_\" + df_plot['lrdecay'].astype(str) + \"_\" + df_plot['batch_size'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "best_bs =32\n",
    "best_lr = 0.01\n",
    "best_decay = 1.0\n",
    "\n",
    "loss_type = \"train\"\n",
    "\n",
    "\n",
    "df_plot = df_centralized_training_loss[\n",
    "    (df_centralized_training_loss['batch_size'] == best_bs) &\n",
    "    (df_centralized_training_loss['lr'] == best_lr) &\n",
    "    (df_centralized_training_loss['lrdecay'] == best_decay)\n",
    "]\n",
    "# combine lr, clipping and batch size into a single column\n",
    "df_plot['lr_decay_bs'] = df_plot['lr'].astype(str) + \"_\" + df_plot['lrdecay'].astype(str) + \"_\" + df_plot['batch_size'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5dec0f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>lr</th>\n",
       "      <th>clipping</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lrdecay</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>lr_decay_bs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>1</td>\n",
       "      <td>2576.834229</td>\n",
       "      <td>0.517930</td>\n",
       "      <td>2556.682792</td>\n",
       "      <td>0.513784</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12601</th>\n",
       "      <td>2</td>\n",
       "      <td>2324.423584</td>\n",
       "      <td>0.466746</td>\n",
       "      <td>2385.601249</td>\n",
       "      <td>0.478795</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12602</th>\n",
       "      <td>3</td>\n",
       "      <td>2370.773682</td>\n",
       "      <td>0.475651</td>\n",
       "      <td>2379.377651</td>\n",
       "      <td>0.477812</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12603</th>\n",
       "      <td>4</td>\n",
       "      <td>2311.869873</td>\n",
       "      <td>0.464138</td>\n",
       "      <td>2365.524902</td>\n",
       "      <td>0.474919</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12604</th>\n",
       "      <td>5</td>\n",
       "      <td>2305.396240</td>\n",
       "      <td>0.462834</td>\n",
       "      <td>2354.089425</td>\n",
       "      <td>0.472137</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12895</th>\n",
       "      <td>296</td>\n",
       "      <td>37.334778</td>\n",
       "      <td>0.006690</td>\n",
       "      <td>39.505858</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12896</th>\n",
       "      <td>297</td>\n",
       "      <td>19.497768</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>18.009737</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12897</th>\n",
       "      <td>298</td>\n",
       "      <td>51.506371</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>56.613772</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12898</th>\n",
       "      <td>299</td>\n",
       "      <td>65.183487</td>\n",
       "      <td>0.012122</td>\n",
       "      <td>34.024928</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12899</th>\n",
       "      <td>300</td>\n",
       "      <td>23.368328</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>24.069212</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lr: 0.01, clipping: False, batch_size: 32, lrd...</td>\n",
       "      <td>0.01_1.0_32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       epoch   train_loss  train_mse     val_loss   val_mse    lr  clipping  \\\n",
       "12600      1  2576.834229   0.517930  2556.682792  0.513784  0.01     False   \n",
       "12601      2  2324.423584   0.466746  2385.601249  0.478795  0.01     False   \n",
       "12602      3  2370.773682   0.475651  2379.377651  0.477812  0.01     False   \n",
       "12603      4  2311.869873   0.464138  2365.524902  0.474919  0.01     False   \n",
       "12604      5  2305.396240   0.462834  2354.089425  0.472137  0.01     False   \n",
       "...      ...          ...        ...          ...       ...   ...       ...   \n",
       "12895    296    37.334778   0.006690    39.505858  0.007195  0.01     False   \n",
       "12896    297    19.497768   0.003047    18.009737  0.002689  0.01     False   \n",
       "12897    298    51.506371   0.009707    56.613772  0.010795  0.01     False   \n",
       "12898    299    65.183487   0.012122    34.024928  0.006139  0.01     False   \n",
       "12899    300    23.368328   0.003924    24.069212  0.004015  0.01     False   \n",
       "\n",
       "       batch_size  lrdecay                                        hyperparams  \\\n",
       "12600          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "12601          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "12602          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "12603          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "12604          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "...           ...      ...                                                ...   \n",
       "12895          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "12896          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "12897          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "12898          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "12899          32      1.0  lr: 0.01, clipping: False, batch_size: 32, lrd...   \n",
       "\n",
       "       lr_decay_bs  \n",
       "12600  0.01_1.0_32  \n",
       "12601  0.01_1.0_32  \n",
       "12602  0.01_1.0_32  \n",
       "12603  0.01_1.0_32  \n",
       "12604  0.01_1.0_32  \n",
       "...            ...  \n",
       "12895  0.01_1.0_32  \n",
       "12896  0.01_1.0_32  \n",
       "12897  0.01_1.0_32  \n",
       "12898  0.01_1.0_32  \n",
       "12899  0.01_1.0_32  \n",
       "\n",
       "[300 rows x 11 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa806968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/1268037728.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "/var/folders/yx/rfhrpk093hz1gfwtn7qxt_180000gq/T/ipykernel_67807/1268037728.py:38: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "\n",
    "# # Plot training loss\n",
    "sns.lineplot(\n",
    "    data=df_plot,\n",
    "    x='epoch',\n",
    "    y='train_loss',\n",
    "    hue='lr_decay_bs',\n",
    "    palette='tab10'\n",
    ")\n",
    "reds = sns.color_palette(\"Reds\", n_colors=1)\n",
    "plt.legend(title=\"lr, decay, batch size\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(left=0, right=20)\n",
    "plt.xticks(ticks=range(0, df_plot['epoch'].max() + 1, 50))\n",
    "plt.title('training loss ')\n",
    "plt.show()\n",
    "# # Plot validation loss\n",
    "plt.figure(figsize=(5, 2.5))\n",
    "sns.lineplot(\n",
    "    data=df_plot,\n",
    "    x='epoch',\n",
    "    y='val_loss',\n",
    "    hue='lr_decay_bs',\n",
    "    palette=reds,\n",
    "    linestyle='-',\n",
    ")\n",
    "\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(left=0, right=20)\n",
    "plt.xticks(ticks=range(0, df_plot['epoch'].max() + 1, 50))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('validation loss ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"lr, decay, batch size\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0b1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1fdb3eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum training loss: 9.22089958190918\n",
      "Minimum validation loss: 10.799880708966937\n"
     ]
    }
   ],
   "source": [
    "# Get the min loss\n",
    "min_train_loss = df_plot['train_loss'].min()\n",
    "min_val_loss = df_plot['val_loss'].min()\n",
    "print(f\"Minimum training loss: {min_train_loss}\")\n",
    "print(f\"Minimum validation loss: {min_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b3243",
   "metadata": {},
   "source": [
    "## Visualize the models predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beacaec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/luisa/Desktop/nygc/cluster/projects/fl4tsf/results_federated_learningtest/periodic/federated_training/FedAvg/100_rounds/rep_1-alpha_0.5-lr_0.01-batchsize_16_clipping_False_lrdecay_1.0_localepochs_10/federated_outputs/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#############################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Load Model and Weigths\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#############################\u001b[39;00m\n\u001b[1;32m      5\u001b[0m weights_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/luisa/Desktop/nygc/cluster/projects/fl4tsf/results_federated_learningtest/periodic/federated_training/FedAvg/100_rounds/rep_1-alpha_0.5-lr_0.01-batchsize_16_clipping_False_lrdecay_1.0_localepochs_10/federated_outputs/model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(weights_file, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Net()\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(weights)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/luisa/Desktop/nygc/cluster/projects/fl4tsf/results_federated_learningtest/periodic/federated_training/FedAvg/100_rounds/rep_1-alpha_0.5-lr_0.01-batchsize_16_clipping_False_lrdecay_1.0_localepochs_10/federated_outputs/model.pth'"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Load Model and Weigths\n",
    "#############################\n",
    "\n",
    "weights_file = \"../results_federated_learningtest/periodic/federated_training/FedAvg/100_rounds/rep_1-alpha_0.5-lr_0.001-batchsize_32_clipping_False_lrdecay_1.0_localepochs_3/federated_outputs/model.pth\"\n",
    "weights = torch.load(weights_file, weights_only=True)\n",
    "model = Net()\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "#############################\n",
    "# Load Dataset\n",
    "#############################\n",
    "test_dataset_filename = \"/Users/luisa/Desktop/nygc/cluster/projects/fl4tsf/data/periodic/periodic_test.pt\"\n",
    "time_steps_filename = \"/Users/luisa/Desktop/nygc/cluster/projects/fl4tsf/data/periodic/periodic_time_steps.pt\"\n",
    "timestamps = torch.load(time_steps_filename, weights_only=True)\n",
    "dataset = torch.load(test_dataset_filename, weights_only=True)\n",
    "\n",
    "plot_n_outputs(model, dataset, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b617c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983adc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aac580",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_n_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_n_outputs(model, dataset, timestamps)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_n_outputs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd61dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbccfe32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8699c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
